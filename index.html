<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kevin Li</title>
    
    <meta name="author" content="Kevin Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="./images/prof_pic.png">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <script src="index.js"></script>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kevin (Yu-Teng) Li
                </p>
                <p>
                  I am an Applied Research Scientist at Adobe Firefly, working on foundation model training and <a href="https://thekevinli.github.io/unifusion/">multimodal research</a>. Most recently I co-led the multimodal pretraining of Firefly Image 5, which enables textual editing, single-image reference generation, layer generation and text-to-image. 
                </p>
                <p>
                  Previously, I graduated from University of California, Berkeley with a B.S. in Electrical Engineering and Computer Sciences, where I did my research in <a href="https://arxiv.org/abs/2306.11180">Active Learning in Segmentation</a> under the supervision of <a href="https://darrellgroup.github.io">Trevor Darrell</a>.
                </p>
                <!-- <p><strong>This website currently is still under construction! 🚧</strong></p> -->
                <p style="text-align:center">
                  <a href="mailto:yutengli@berkeley.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/resume.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="">Google Scholar</a> &nbsp;/&nbsp; -->
                  <a href="https://x.com/curiouskid423">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yutengli/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/thekevinli">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/prof_pic.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 3%;" alt="profile photo" src="images/prof_pic.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px 0;width:100%;vertical-align:middle">
                <h2>Research & Industry Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr style="background-color: #ffffd0;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/unifusion_arch.png" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://thekevinli.github.io/unifusion/">
                  <span class="papertitle">UniFusion: Vision-Language Model as Unified Encoder for Image Generation</span>
                </a>
                <br>
                <strong>Yu-Teng Li*</strong>,
                <a href="">Manuel Brack*</a>,
                <a href="">Sudeep Katakol</a>,
                <a href="">Hareesh Ravi</a>,
                <a href="">Ajinkya Kale</a>
                <br>
                <em>ArXiv</em>, 2025
                <p></p>
                <p>
                  The first architecture that uses only VLM as input-condition encoder without auxiliary signals from VAE or CLIP to do editing. The unified encoder framework enables emergent capabilities such as zero-shot multi-ref generation when trained on single-reference pairs.
                </p>
              </td>
            </tr>

            <tr style="background-color: #ffffd0;">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/firefly_image_5.png" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">Firefly Image 5</span>
                </a>
                <br>
                <span style="padding: 3px 0; display: block;"><strong>Multimodal pretraining</strong> (textual editing, single-reference generation, layer generation)</span>
                <span><em>October 2025</em></span>
                <p></p>
                <p>
                  I co-led the training of Firefly Image 5 model for workflows of textual editing, single-image reference and layer generation. Throughout model development, I led ablation studies on the architecture and data combinations, and drove decisions on the final production model's training recipe, scaling to ~1000-GPU distributed training on a daily basis (July-Oct 2025).
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/firefly_image_4.jpg" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://blog.adobe.com/en/publish/2025/04/24/adobe-firefly-next-evolution-creative-ai-is-here">
                  <span class="papertitle">Firefly Image 4</span>
                </a>
                <br>
                <span style="padding: 3px 0; display: block;"><strong>Foundation model pretraining & post-training</strong></span>
                <span><em>April 2025</em></span>
                <p></p>
                <p>
                  Being part of the foundation model training team of Image 4, I developed recipes for synthetic data handling and aesthetics fine-tuning (SFT), as well as sampling improvements. Firefly Image 4 is, as of Oct 2025, still one of the most advanced text-to-image models in the industry, <a href="./images/text_to_image_arena_screenshot_oct2025.png", target="_blank">leading competitors</a> such as Qwen-Image, Runway Gen-4 Image, Luma Photon...etc in "General & Photorealistic" category on <a href="https://artificialanalysis.ai/image/leaderboard/text-to-image?style=general-and-photorealistic">Text-to-Image Arena</a>.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/firefly_image_3.png" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://helpx.adobe.com/firefly/web/work-with-enterprise-features/train-custom-models/custom-models-overview.html">
                  <span class="papertitle">Firefly Image 3 Custom Models</span>
                </a>
                <br>
                <span><em>August 2024</em></span>
                <p></p>
                <p>
                  I led the personalization effort of Firefly Image 3 called Custom Models, which enables copyright content generations for Adobe's enterprise customers. I developed the training recipe (e.g. improved Dreambooth's stability with VLM-predicted superclass, improve optimizer memory efficiency) and integrated the finetuning pipeline into prodcution. 
                </p>
              </td>
            </tr>
          
          

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/halo.png" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2306.11180">
                  <span class="papertitle">Hyperbolic Active Learning for Semantic Segmentation under Domain Shift</span>
                </a>
                <br>
                <a href="">Luca Franco*</a>,
                <a href="">Paolo Mandica*</a>,
                <a href="">Konstantinos Kallidromitis</a>,
                <a href="">Devin Guillory</a>,
                <strong>Yu-Teng Li</strong>,
                <a href="">Trevor Darrell</a>,
                <a href="">Fabio Galasso</a>
                <br>
                <em>ICML</em>, 2024
                <br>
                <p></p>
                <p>
                  HALO introduces a hyperbolic neural network approach to pixel-based active learning (AL) for semantic segmentation, and is the first active AL method to surpass the performance of supervised domain adaptation with just 1% of labeled pixels.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one; display: block; margin: auto">
                  <img src="images/lunar_lander.gif" width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2212.10712">
                  <span class="papertitle">Neighboring state-based RL Exploration</span>
                </a>
                <br>
                <a href="">Jeffery Cheng*</a>,
                <strong>Yu-Teng Li*</strong>,
                <a href="">Justin Lin*</a>,
                <a href="">Pedro Pachuca*</a>
                <br>
                <em>ArXiv</em>, 2022
                <br>
                <p></p>
                <p>
                  We propose ρ-explore, a model-free exploration algorithm that selects actions based on nearby perturbed states, which consistently outperforms the Double DQN baseline in an discrete environment by 49% in terms of validation reward return [<a href="https://github.com/thekevinli/rho_exploration">code</a>]. 
                </p>
              </td>
            </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Teaching</h2>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one; display: block; margin: auto">
            <img src="images/cs182.png" width=100%>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://inst.eecs.berkeley.edu/~cs182/sp23/">
            <span class="papertitle">CS 182/282A&nbsp;&nbsp;Deep Neural Networks&nbsp;&nbsp;|&nbsp;&nbsp;UC Berkeley</span>
          </a>
          <br>
          <spans style="padding:3px 0;"><strong>Head Teaching Assistant of Discussions</strong></span>
          <br>
          <p></p>
          <p>
            I <a href="./images/cs182_course_staff.png">led the curriculum design</a> of weekly discussion sections in the Deep Learning course at UC Berkeley, with 300+ graduate & undergraduate students in Spring 2023. I also designed various exam and homework questions on denoising diffusion models (DDPM), Transformers, and more.
          </p>
        </td>
      </tr>
    </tbody></table>
    
          <!--
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
        -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website source code from <a href="https://github.com/jonbarron/jonbarron_website">here</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
